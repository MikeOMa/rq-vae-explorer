# RQ-VAE Explorer Design

Interactive training visualization tool for exploring RQ-VAE loss functions.

## Overview

A browser-based tool that lets you:
- Train an RQ-VAE on MNIST with a 2D latent space
- Watch codebook centers evolve in real-time
- Adjust loss function weights during training
- Monitor codebook health (active vs dead vectors)

## Stack

- **JAX + Flax** â€” model implementation
- **Gradio** â€” browser UI
- **uv** â€” package management
- **MNIST** â€” dataset (via tensorflow-datasets)

## Project Structure

```
rq_vae_loss_exploration/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ src/
â”‚   â””â”€â”€ rq_vae_explorer/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ model/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ encoder.py      # Flax CNN encoder â†’ 2D latent
â”‚       â”‚   â”œâ”€â”€ decoder.py      # 2D latent â†’ 28x28 reconstruction
â”‚       â”‚   â”œâ”€â”€ quantizer.py    # Residual quantization logic
â”‚       â”‚   â””â”€â”€ rqvae.py        # Combines encoder + quantizer + decoder
â”‚       â”œâ”€â”€ training/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ losses.py       # Reconstruction, commitment, codebook losses
â”‚       â”‚   â”œâ”€â”€ trainer.py      # Training loop with JAX jit
â”‚       â”‚   â””â”€â”€ state.py        # Shared mutable state for UI â†” trainer
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ mnist.py        # MNIST dataset loading
â”‚       â””â”€â”€ ui/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ app.py          # Gradio app entry point
â”‚           â”œâ”€â”€ plots.py        # 2D codebook + latent visualization
â”‚           â””â”€â”€ controls.py     # Loss weight sliders
â””â”€â”€ scripts/
    â””â”€â”€ run.py
```

## Model Architecture

### Encoder (MNIST 28x28 â†’ 2D latent)

```
Input: (batch, 28, 28, 1)
  â†’ Conv 32 filters, 3x3, stride 2, ReLU  â†’ (batch, 14, 14, 32)
  â†’ Conv 64 filters, 3x3, stride 2, ReLU  â†’ (batch, 7, 7, 64)
  â†’ Flatten â†’ Dense 128 â†’ ReLU
  â†’ Dense 2  â†’ (batch, 2)
```

### Decoder (2D latent â†’ 28x28 reconstruction)

```
Input: (batch, 2)
  â†’ Dense 128 â†’ ReLU
  â†’ Dense 7*7*64 â†’ Reshape â†’ (batch, 7, 7, 64)
  â†’ ConvTranspose 32 filters, 3x3, stride 2, ReLU â†’ (batch, 14, 14, 32)
  â†’ ConvTranspose 1 filter, 3x3, stride 2, Sigmoid â†’ (batch, 28, 28, 1)
```

### Quantizer (residual quantization)

```
For each of D levels (default 2):
  1. Find nearest codebook vector to current residual
  2. Quantized output += codebook vector
  3. Residual = input - quantized output (for next level)

Codebook shape: (D, K, 2) â†’ D levels Ã— K=16 vectors Ã— 2D
```

Straight-through estimator for gradients (gradients pass through quantization as identity).

**Default configuration:**
- K = 16 codebook vectors per level
- D = 2 residual quantization levels
- Both configurable via UI

## Loss Functions

```python
total_loss = recon_loss + Î»_commit * commit_loss + Î»_codebook * codebook_loss
```

| Loss | Formula | Purpose |
|------|---------|---------|
| Reconstruction | `MSE(input, reconstructed)` | Output quality |
| Commitment | `MSE(z_e, stop_gradient(z_q))` | Encoder commits to codebook |
| Codebook | `MSE(stop_gradient(z_e), z_q)` | Codebook moves toward encoder |

**Default weights:**
- `Î»_commit = 0.25`
- `Î»_codebook = 1.0`

Weights adjustable via UI sliders during training â€” changes take effect on the next batch.

## UI Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [Start Training] [Pause] [Reset]            Step: 1234     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         â”‚                                   â”‚
â”‚   2D Codebook Plot      â”‚   Sample Reconstructions          â”‚
â”‚   â— Active centers      â”‚   (Input â†’ Reconstructed)         â”‚
â”‚   â—‹ Dead centers        â”‚                                   â”‚
â”‚   Â· Encoder outputs     â”‚                                   â”‚
â”‚     (colored by digit)  â”‚                                   â”‚
â”‚                         â”‚                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Codebook Health        â”‚  Loss Plot (over time)            â”‚
â”‚  Level 1: 14/16 active  â”‚  ğŸ“‰ recon â”€â”€ commit â”€â”€ codebook   â”‚
â”‚  Level 2: 12/16 active  â”‚                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Î»_commit:    [====â—‹================] 0.25                  â”‚
â”‚  Î»_codebook:  [==========â—‹==========] 1.00                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Codebook health tracking

- Rolling window (last 100 batches) of assignment counts per vector
- Vector is "dead" if <1% of assignments in that window
- Displayed both visually (â—‹ vs â—) and in stats panel

## Training Loop & UI Interaction

**Training runs in a background thread:**

```python
while training:
    batch = next(data_iterator)

    # Read current weights from shared state
    lambdas = state.get_lambdas()

    # JIT-compiled train step
    params, opt_state, metrics, codebook = train_step(
        params, opt_state, batch, lambdas
    )

    # Update shared state for UI
    state.update(
        codebook=codebook,
        losses=metrics,
        step=step,
        assignments=assignments,
    )

    # Sample reconstructions every 50 steps
    if step % 50 == 0:
        state.update(reconstructions=reconstruct(params, sample_batch))
```

**UI polling:** Gradio refreshes plots every 500ms while training.

**Thread safety:** Python threading with locks. JAX releases GIL during computation, so this works without multiprocessing.

## Dependencies

```toml
[project]
name = "rq-vae-explorer"
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "jax[cpu]",
    "flax",
    "optax",
    "tensorflow-datasets",
    "gradio",
    "matplotlib",
    "numpy",
]

[project.optional-dependencies]
gpu = ["jax[cuda12]"]

[project.scripts]
rq-vae-explorer = "rq_vae_explorer.ui.app:main"
```

## Running

```bash
# Setup
uv sync

# Run (CPU)
uv run rq-vae-explorer

# With GPU
uv sync --extra gpu
uv run rq-vae-explorer
```

## Future Considerations

### Additional tunable parameters (planned)

- Codebook size (K) and quantization depth (D) adjustable via UI
- Learning rate
- Other loss formulas (MSE vs BCE, EMA updates vs gradient-based codebook learning)

### Dead codebook remediation (planned)

Currently: detection and visualization only.

Future experiments:
- Random reinitialization from encoder outputs
- Splitting popular codebook vectors
- Entropy regularization to encourage uniform usage
- EMA decay thresholds

The quantizer module is isolated specifically to accommodate these experiments without disrupting the rest of the codebase.
